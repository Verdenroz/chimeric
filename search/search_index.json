{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Chimeric","text":"<p>Chimeric is a unified Python interface for multiple LLM providers with automatic provider detection and seamless switching.</p>"},{"location":"#setup","title":"Setup","text":"<p>Chimeric provides a unified interface for 7 major AI providers:</p> <p> </p> <p>Each provider can be installed individually or together using extras:</p> <pre><code># Individual providers\npip install \"chimeric[openai]\"\npip install \"chimeric[anthropic]\"\npip install \"chimeric[google]\"\npip install \"chimeric[cohere]\"\npip install \"chimeric[groq]\"\npip install \"chimeric[cerebras]\"\npip install \"chimeric[grok]\"\n\n# Multiple providers\npip install \"chimeric[openai,anthropic,google]\"\n\n# All providers\npip install \"chimeric[all]\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code>from chimeric import Chimeric\n\nclient = Chimeric()  # Auto-detects API keys from environment\n\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"Hello!\"\n)\nprint(response.content)\n</code></pre>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Multi-Provider Switching:</p> <pre><code># Seamlessly switch between providers - string input\ngpt_response = client.generate(model=\"gpt-4o\", messages=\"Explain quantum computing\")\nclaude_response = client.generate(model=\"claude-3-5-haiku-latest\", messages=\"Write a poem about AI\")\ngemini_response = client.generate(model=\"gemini-2.5-flash\", messages=\"Summarize climate change\")\n</code></pre> <p>Flexibility:</p> <pre><code># Mixed usage in same application\nunified = client.generate(model=\"claude-3-5-haiku-latest\", messages=\"Code review this function\")\nnative = client.generate(model=\"claude-3-5-haiku-latest\", messages=\"Debug this error\", native=True)\n\n# Use unified for consistent cross-provider code\nprint(unified.content)\n\n# Use native for provider-specific features\nif hasattr(native, 'stop_reason'):\n    print(f\"Claude stop reason: {native.stop_reason}\")\n</code></pre> <p>Streaming:</p> <pre><code>stream = client.generate(\n    model=\"gpt-4o\",\n    messages=\"Tell me a story about space exploration\",\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Function Calling:</p> <pre><code>@client.tool()\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    return f\"Sunny, 72\u00b0F in {city}\"\n\n\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in NYC?\"}]\n)\n</code></pre> <p>Async Support:</p> <pre><code>import asyncio\n\n\nasync def main():\n    response = await client.agenerate(\n        model=\"claude-3-5-sonnet-latest\",\n        messages=[{\"role\": \"user\", \"content\": \"Analyze this data\"},\n                  {\"role\": \"assistant\",\n                   \"content\": \"I'd be happy to help analyze data. What data would you like me to look at?\"},\n                  {\"role\": \"user\", \"content\": \"Sales figures from Q4\"}]\n    )\n    print(response.content)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"#known-limitations","title":"Known Limitations","text":"<ul> <li>Beta Status: API may change as we refine the interface</li> <li>Provider Dependencies: Each provider requires separate installation extra</li> <li>Rate Limits: Subject to individual provider rate limits and quotas</li> <li>Multimodal Support: Image and audio support is untested and may vary by provider</li> <li>Model Availability: Some models may not be available in all regions</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Embeddings Support: Unified interface for text embeddings across providers</li> <li>Mutli-Modal Support: Enhanced support for images and audio</li> <li>Cost Tracking: Built-in usage and cost monitoring</li> <li>Advanced Routing: Load balancing and failover between providers</li> </ul> <p>License: Chimeric is licensed under the MIT License.</p>"},{"location":"contributing/","title":"Contributor Guide","text":"<p>Thank you for your interest in improving this project. This project is open-source under the MIT license and welcomes contributions in the form of bug reports, feature requests, and pull requests.</p>"},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>Source Code</li> <li>Documentation</li> <li>Issue Tracker</li> <li>Code of Conduct</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Use our Bug Report template which will guide you through providing: - Python version and environment details - Minimal code example to reproduce the issue - Expected vs. actual behavior - Relevant error messages and stack traces - Affected providers</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Use our Feature Request template to provide: - Clear description of the proposed feature - Problem statement and motivation - Use cases and benefits - Proposed API design with examples</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>Requirements: Python 3.11, 3.12, 3.13 (use pyenv for multiple versions)</p> <pre><code># Install Python versions\npyenv install 3.11 3.12 3.13\n\n# Install development tools\npip install uv nox\n\n# Install package with dev dependencies\nmake install\n# or: uv sync --all-extras --dev\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code># Run all tests\nmake test\n\n# Run unit tests\nmake test-unit\n\n# Run integration tests\nmake test-integration\n\n# Cross-version testing\nmake nox\n</code></pre>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<p>Requirements for acceptance:</p> <ul> <li>All tests pass</li> <li>Maintain 100% code coverage</li> <li>Update documentation for new features</li> <li>Follow code style (run <code>make lint</code>)</li> </ul> <p>Process:</p> <ol> <li>Open an issue to discuss your approach</li> <li>Fork and create a feature branch</li> <li>Make your changes with tests</li> <li>Run <code>make lint</code> before committing</li> <li>Open a pull request</li> </ol>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>This guide covers the various ways to configure Chimeric for your specific needs.</p>"},{"location":"usage/configuration/#api-key-configuration","title":"API Key Configuration","text":""},{"location":"usage/configuration/#environment-variables-recommended","title":"Environment Variables (Recommended)","text":"<p>The simplest way to configure API keys is through environment variables:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Google (supports both variable names)\nexport GOOGLE_API_KEY=\"AIza...\"\n# or\nexport GEMINI_API_KEY=\"AIza...\"\n\n# Cohere (supports both variable names)\nexport COHERE_API_KEY=\"your-key\"\n# or  \nexport CO_API_KEY=\"your-key\"\n\n# Groq\nexport GROQ_API_KEY=\"gsk_...\"\n\n# Cerebras\nexport CEREBRAS_API_KEY=\"csk-...\"\n\n# Grok (supports both variable names)\nexport GROK_API_KEY=\"xai-...\"\n# or\nexport GROK_API_TOKEN=\"xai-...\"\n</code></pre>"},{"location":"usage/configuration/#direct-initialization","title":"Direct Initialization","text":"<p>You can also provide API keys directly when creating the client:</p> <pre><code>from chimeric import Chimeric\n\nclient = Chimeric(\n    openai_api_key=\"sk-...\",\n    anthropic_api_key=\"sk-ant-...\",\n    google_api_key=\"AIza...\",\n    cohere_api_key=\"your-key\",\n    groq_api_key=\"gsk_...\",\n    cerebras_api_key=\"csk-...\",\n    grok_api_key=\"xai-...\"\n)\n</code></pre>"},{"location":"usage/configuration/#mixed-configuration","title":"Mixed Configuration","text":"<p>Combine environment variables with direct initialization:</p> <pre><code># Some keys from environment, others provided directly\nclient = Chimeric(\n    openai_api_key=\"sk-...\",  # Explicit key\n    # anthropic_api_key will be read from ANTHROPIC_API_KEY env var\n    # google_api_key will be read from GOOGLE_API_KEY env var\n)\n</code></pre>"},{"location":"usage/configuration/#provider-specific-configuration","title":"Provider-Specific Configuration","text":"<p>Pass any provider-specific configuration options directly to Chimeric. Chimeric uses signature introspection to automatically filter kwargs, ensuring each provider only receives parameters its constructor accepts:</p> <pre><code># All kwargs are passed to every provider, but automatically filtered\nclient = Chimeric(\n    openai_api_key=\"sk-...\",\n    anthropic_api_key=\"sk-ant-...\",\n    # Google-specific parameters (ignored by other providers)\n    vertexai=True,\n)\n</code></pre>"},{"location":"usage/configuration/#common-configuration-options","title":"Common Configuration Options","text":"<p>Parameters that work across multiple providers:</p> <pre><code>client = Chimeric(\n    openai_api_key=\"sk-...\",\n    anthropic_api_key=\"sk-ant-...\",\n    google_api_key=\"AIza...\",\n\n    # HTTP/Connection settings (widely supported)\n    timeout=60,                    # Request timeout\n    max_retries=3,                # Retry configuration\n\n    # Custom endpoints (where supported)\n    base_url=\"https://api.your-company.com/v1\",\n\n    # Headers (where supported)\n    default_headers={\"User-Agent\": \"YourApp/1.0\"},\n)\n</code></pre> <p>Key Benefits: - Signature-Based Filtering: Uses Python introspection to validate parameters - No TypeErrors: Invalid parameters are automatically filtered out - Cross-Provider Compatibility: Same parameters work where applicable - Unified Configuration: Configure all providers with one constructor call</p>"},{"location":"usage/configuration/#provider-configuration","title":"Provider Configuration","text":""},{"location":"usage/configuration/#provider-information","title":"Provider Information","text":"<p>Get information about configured providers:</p> <pre><code>client = Chimeric()\n\n# List all configured providers\nprint(\"Available providers:\", client.available_providers)\n\n# Check capabilities across all providers\nprint(\"Merged capabilities:\", client.capabilities)\n\n# Check capabilities for a specific provider\nif \"openai\" in client.available_providers:\n    openai_caps = client.get_capabilities(\"openai\")\n    print(f\"OpenAI capabilities: {openai_caps}\")\n</code></pre>"},{"location":"usage/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"usage/configuration/#model-discovery","title":"Model Discovery","text":"<p>Discover available models across providers:</p> <pre><code># List all models from all providers\nall_models = client.list_models()\nfor model in all_models:\n    print(f\"{model.id} ({model.provider})\")\n\n# List models from a specific provider\nopenai_models = client.list_models(\"openai\")\nfor model in openai_models:\n    print(f\"OpenAI: {model.id}\")\n</code></pre>"},{"location":"usage/getting-started/","title":"Getting Started","text":"<p>Chimeric provides a unified interface for accessing multiple LLM providers through a single, consistent API. This guide demonstrates fundamental concepts and usage patterns to get you productive quickly.</p>"},{"location":"usage/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>API keys for your chosen providers</li> <li>Basic familiarity with async/await patterns (for async usage)</li> </ul>"},{"location":"usage/getting-started/#core-concepts","title":"Core Concepts","text":""},{"location":"usage/getting-started/#automatic-provider-detection","title":"Automatic Provider Detection","text":"<p>Chimeric intelligently routes requests based on model names, eliminating the need for manual provider selection:</p> <pre><code>from chimeric import Chimeric\n\nclient = Chimeric()  # Auto-detects available providers from environment\n\n# Each model automatically routes to its respective provider\ngpt_response = client.generate(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Analyze market trends\"}]\n)\n\n# Messages can also be provided as a string\nclaude_response = client.generate(\n    model=\"claude-3-5-haiku-latest\", \n    messages=\"Hello, world!\"\n)\n\ngemini_response = client.generate(\n    model=\"gemini-2.5-flash\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize research findings\"}]\n)\n</code></pre>"},{"location":"usage/getting-started/#unified-response-format","title":"Unified Response Format","text":"<p>All providers return responses in a consistent format:</p> <pre><code>response = client.generate(model=\"gpt-4o\", messages=[...])\n\n# Common interface\nprint(response.content)              # Text content\nprint(response.model)                # Model used\nprint(response.usage.prompt_tokens)  # Input token usage\nprint(response.usage.completion_tokens)  # Output token usage\nprint(response.usage.total_tokens)   # Total tokens\n\n# Access native provider response with native=True parameter\nnative_response = client.generate(\n    model=\"claude-3-5-haiku-latest\", \n    messages=[...], \n    native=True\n)\nprint(native_response.stop_reason)  # Anthropic-specific stop reason\n</code></pre>"},{"location":"usage/getting-started/#streaming-responses","title":"Streaming Responses","text":"<p>Stream responses token-by-token for real-time applications:</p> <pre><code>stream = client.generate(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.content:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"usage/getting-started/#function-registration-and-execution","title":"Function Registration and Execution","text":"<p>Chimeric provides a decorator-based system for registering functions that models can invoke:</p> <pre><code>client = Chimeric()\n\n@client.tool()\ndef analyze_financial_data(\n    symbol: str, \n    period: str = \"1y\", \n    metrics: list[str] = None\n) -&gt; dict:\n    \"\"\"Analyze financial performance metrics for a given symbol.\n\n    Args:\n        symbol: Stock symbol (e.g., 'AAPL', 'MSFT')\n        period: Analysis period ('1y', '6m', '3m')\n        metrics: Specific metrics to analyze\n\n    Returns:\n        Dict containing analysis results and recommendations\n    \"\"\"\n    return {\n        \"symbol\": symbol,\n        \"period\": period,\n        \"performance\": \"positive\",\n        \"volatility\": \"moderate\",\n        \"recommendation\": \"hold\"\n    }\n\n# Functions are automatically available to all compatible models\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Analyze Tesla's performance over the last year\"}]\n)\n</code></pre>"},{"location":"usage/getting-started/#async-support","title":"Async Support","text":"<p>Chimeric supports async operations for high-performance applications:</p> <pre><code>import asyncio\nfrom chimeric import Chimeric\n\nasync def main():\n    client = Chimeric()\n\n    response = await client.agenerate(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n\n    print(response.content)\n\n# Async streaming\nasync def stream_example():\n    client = Chimeric()\n\n    stream = await client.agenerate(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.content:\n            print(chunk.content, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"usage/getting-started/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully across providers:</p> <pre><code>from chimeric import Chimeric\nfrom chimeric.exceptions import ChimericError\n\nclient = Chimeric()\n\ntry:\n    response = client.generate(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\nexcept ChimericError as e:\n    print(f\"Chimeric error: {e}\")\n</code></pre>"},{"location":"usage/responses/","title":"Response Types and Formats","text":"<p>Chimeric provides a unique dual response system that gives you the best of both worlds: unified consistency across providers and access to provider-specific features when needed.</p>"},{"location":"usage/responses/#overview","title":"Overview","text":"<p>Every response from Chimeric contains two formats:</p> <ul> <li>Unified Format (default): Standardized <code>CompletionResponse</code> with consistent fields across all providers</li> <li>Native Format: Provider's original response object with all provider-specific fields and metadata</li> </ul> <p>This dual system allows you to write cross-provider code while still accessing provider-specific features when necessary.</p>"},{"location":"usage/responses/#response-architecture","title":"Response Architecture","text":""},{"location":"usage/responses/#internal-structure","title":"Internal Structure","text":"<p>Internally, Chimeric wraps all provider responses in container objects:</p> <pre><code># Non-streaming responses\nChimericCompletionResponse[NativeType]:\n    .native   # Provider-specific response object\n    .common   # Unified CompletionResponse format\n\n# Streaming responses  \nChimericStreamChunk[NativeType]:\n    .native   # Provider-specific chunk object\n    .common   # Unified StreamChunk format\n</code></pre>"},{"location":"usage/responses/#access-control","title":"Access Control","text":"<p>The <code>native</code> parameter controls which format you receive:</p> <pre><code># Default: Returns unified format\nresponse = client.generate(model=\"gpt-4o\", messages=\"Hello\")\n# Type: CompletionResponse\n\n# Native: Returns provider-specific format\nnative_response = client.generate(model=\"gpt-4o\", messages=\"Hello\", native=True)  \n# Type: OpenAI's ChatCompletion object\n</code></pre>"},{"location":"usage/responses/#unified-format-default","title":"Unified Format (Default)","text":"<p>The unified format provides consistent fields across all providers:</p>"},{"location":"usage/responses/#completionresponse-structure","title":"CompletionResponse Structure","text":"<pre><code>from chimeric import Chimeric\n\nclient = Chimeric()\nresponse = client.generate(model=\"gpt-4o\", messages=\"Explain quantum physics\")\n\n# Standardized fields available across all providers\nprint(response.content)           # str | list[Any] - Main response content\nprint(response.model)             # str | None - Model that generated response  \nprint(response.usage.prompt_tokens)      # int - Input tokens used\nprint(response.usage.completion_tokens)  # int - Output tokens generated\nprint(response.usage.total_tokens)       # int - Total tokens\nprint(response.metadata)          # dict[str, Any] | None - Additional info\n</code></pre>"},{"location":"usage/responses/#streamchunk-structure-streaming","title":"StreamChunk Structure (Streaming)","text":"<pre><code>stream = client.generate(\n    model=\"gpt-4o\", \n    messages=\"Write a story\", \n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk.content)          # str | list[Any] - Accumulated content\n    print(chunk.delta)            # str | None - New content in this chunk\n    print(chunk.finish_reason)    # str | None - Reason streaming stopped\n    print(chunk.metadata)         # dict[str, Any] | None - Chunk metadata\n</code></pre>"},{"location":"usage/responses/#cross-provider-consistency","title":"Cross-Provider Consistency","text":"<p>The unified format ensures your code works identically across providers:</p> <pre><code>def analyze_with_any_model(model_name: str, text: str) -&gt; str:\n    \"\"\"Works with any provider - OpenAI, Anthropic, Google, etc.\"\"\"\n    response = client.generate(\n        model=model_name,\n        messages=f\"Analyze this text: {text}\"\n    )\n\n    # Same interface regardless of provider\n    tokens_used = response.usage.total_tokens\n    content = response.content\n\n    return f\"Analysis ({tokens_used} tokens): {content}\"\n\n# Works with any model/provider\nresult1 = analyze_with_any_model(\"gpt-4o\", \"Sample text\")\nresult2 = analyze_with_any_model(\"claude-3-5-sonnet-20241022\", \"Sample text\") \nresult3 = analyze_with_any_model(\"gemini-1.5-pro\", \"Sample text\")\n</code></pre>"},{"location":"usage/responses/#when-to-use-each-format","title":"When to Use Each Format","text":""},{"location":"usage/responses/#use-unified-format-when","title":"Use Unified Format When:","text":"<ul> <li>Cross-provider compatibility is important</li> <li>Building provider-agnostic applications</li> <li>You only need standard fields (content, usage, model)</li> <li>Consistency across different models/providers is required</li> <li>Building generic tools or libraries</li> </ul> <pre><code># Perfect for cross-provider applications\ndef summarize_text(text: str, model: str) -&gt; dict:\n    response = client.generate(model=model, messages=f\"Summarize: {text}\")\n    return {\n        \"summary\": response.content,\n        \"tokens_used\": response.usage.total_tokens,\n        \"model\": response.model\n    }\n</code></pre>"},{"location":"usage/responses/#use-native-format-when","title":"Use Native Format When:","text":"<ul> <li>You need provider-specific metadata (IDs, timestamps, safety ratings)</li> <li>Accessing unique provider features (stop sequences, system fingerprints)</li> <li>Debugging or logging detailed response information</li> <li>Integration with provider-specific tools or workflows</li> <li>Advanced monitoring of provider-specific metrics</li> </ul> <pre><code># Perfect for detailed logging and monitoring\ndef detailed_completion_log(prompt: str, model: str):\n    native_response = client.generate(model=model, messages=prompt, native=True)\n\n    # Log provider-specific details for debugging\n    if \"gpt\" in model:\n        log_openai_response(native_response)\n    elif \"claude\" in model:\n        log_anthropic_response(native_response)\n</code></pre>"},{"location":"usage/responses/#async-support","title":"Async Support","text":"<p>Both formats work identically with async operations:</p> <pre><code>import asyncio\n\nasync def main():\n    # Unified async response\n    response = await client.agenerate(model=\"gpt-4o\", messages=\"Hello\")\n    print(response.content)\n\n    # Native async response  \n    native_response = await client.agenerate(\n        model=\"gpt-4o\", \n        messages=\"Hello\", \n        native=True\n    )\n    print(native_response.choices[0].message.content)\n\n    # Unified async streaming\n    stream = await client.agenerate(model=\"gpt-4o\", messages=\"Story\", stream=True)\n    async for chunk in stream:\n        if chunk.delta:\n            print(chunk.delta, end=\"\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"usage/responses/#best-practices","title":"Best Practices","text":""},{"location":"usage/responses/#start-with-unified-format","title":"Start with Unified Format","text":"<pre><code># Begin with unified format for simplicity\nresponse = client.generate(model=\"gpt-4o\", messages=\"Hello\")\ncontent = response.content\ntokens = response.usage.total_tokens\n</code></pre>"},{"location":"usage/responses/#switch-to-native-when-needed","title":"Switch to Native When Needed","text":"<pre><code># Use native format only when you need provider-specific features\nif need_detailed_metadata:\n    native_response = client.generate(model=\"gpt-4o\", messages=\"Hello\", native=True)\n    response_id = native_response.id\n    created_time = native_response.created\n</code></pre>"},{"location":"usage/responses/#handle-multiple-providers-gracefully","title":"Handle Multiple Providers Gracefully","text":"<pre><code>def smart_response_handler(model: str, prompt: str):\n    # Use unified for basic info\n    response = client.generate(model=model, messages=prompt)\n    result = {\"content\": response.content, \"tokens\": response.usage.total_tokens}\n\n    # Add provider-specific details if needed\n    if need_provider_details:\n        native = client.generate(model=model, messages=prompt, native=True)\n        result[\"native_metadata\"] = extract_provider_metadata(native, model)\n\n    return result\n</code></pre> <p>This dual response system ensures you can build both flexible cross-provider applications and provider-specific integrations with the same codebase.</p>"},{"location":"usage/streaming/","title":"Streaming Responses","text":"<p>Chimeric provides comprehensive streaming support that allows you to receive AI model responses in real-time as they are generated, rather than waiting for the complete response. This is particularly useful for interactive applications, chatbots, and scenarios where you want to display responses progressively.</p>"},{"location":"usage/streaming/#overview","title":"Overview","text":"<p>Streaming enables token-by-token delivery of responses, providing immediate feedback to users and creating more responsive applications. Chimeric's streaming system:</p> <ul> <li>Unified Interface: Same streaming API across all providers</li> <li>Dual Format Support: Access both unified and native streaming formats</li> <li>Advanced Features: Tool call streaming and multi-turn conversations</li> <li>State Management: Automatic content accumulation and metadata handling</li> </ul>"},{"location":"usage/streaming/#basic-streaming","title":"Basic Streaming","text":""},{"location":"usage/streaming/#simple-text-streaming","title":"Simple Text Streaming","text":"<p>Enable streaming by setting <code>stream=True</code>:</p> <pre><code>from chimeric import Chimeric\n\nclient = Chimeric()\n\n# Basic streaming\nstream = client.generate(\n    model=\"gpt-4o\",\n    messages=\"Tell me a story about space exploration\",\n    stream=True\n)\n\n# Process chunks in real-time\nfor chunk in stream:\n    if chunk.delta:  # New content in this chunk\n        print(chunk.delta, end=\"\", flush=True)\n\n    if chunk.finish_reason:\n        print(f\"\\nStreaming finished: {chunk.finish_reason}\")\n</code></pre>"},{"location":"usage/streaming/#understanding-stream-chunks","title":"Understanding Stream Chunks","text":"<p>Each stream chunk contains several fields:</p> <pre><code>stream = client.generate(\n    model=\"gpt-4o\",\n    messages=\"Explain quantum physics briefly\",\n    stream=True\n)\n\nfor chunk in stream:\n    print(f\"Content: {chunk.content}\")      # Accumulated text so far\n    print(f\"Delta: {chunk.delta}\")          # New text in this chunk\n    print(f\"Finish: {chunk.finish_reason}\") # Why streaming stopped (if finished)\n    print(f\"Meta: {chunk.metadata}\")        # Additional chunk info\n    print(\"---\")\n</code></pre>"},{"location":"usage/streaming/#stream-chunk-fields","title":"Stream Chunk Fields","text":""},{"location":"usage/streaming/#content","title":"content","text":"<p>The accumulated text content from the start of the response up to the current chunk:</p> <pre><code>accumulated_text = \"\"\nfor chunk in stream:\n    # chunk.content contains all text so far\n    accumulated_text = chunk.content\n    print(f\"Total so far: {accumulated_text}\")\n</code></pre>"},{"location":"usage/streaming/#delta","title":"delta","text":"<p>The incremental text added in this specific chunk:</p> <pre><code>full_response = \"\"\nfor chunk in stream:\n    if chunk.delta:\n        full_response += chunk.delta  # Build response incrementally\n        print(chunk.delta, end=\"\")    # Display new text immediately\n</code></pre>"},{"location":"usage/streaming/#finish_reason","title":"finish_reason","text":"<p>Indicates why the stream ended (typically in the final chunk):</p> <pre><code>for chunk in stream:\n    if chunk.finish_reason:\n        print(f\"Stream ended: {chunk.finish_reason}\")\n        # Common values: \"stop\", \"length\", \"tool_calls\", \"content_filter\"\n</code></pre>"},{"location":"usage/streaming/#metadata","title":"metadata","text":"<p>Contains additional information about the chunk or final response:</p> <pre><code>for chunk in stream:\n    if chunk.metadata:\n        print(f\"Chunk metadata: {chunk.metadata}\")\n        # May include: token counts, model info, request IDs, etc.\n</code></pre>"},{"location":"usage/streaming/#async-streaming","title":"Async Streaming","text":"<p>Use async streaming for high-performance applications:</p> <pre><code>import asyncio\n\nasync def stream_example():\n    client = Chimeric()\n\n    stream = await client.agenerate(\n        model=\"gpt-4o\",\n        messages=\"Write a poem about artificial intelligence\",\n        stream=True\n    )\n\n    async for chunk in stream:\n        if chunk.delta:\n            print(chunk.delta, end=\"\", flush=True)\n\n        if chunk.finish_reason:\n            print(f\"\\nFinished: {chunk.finish_reason}\")\n\nasyncio.run(stream_example())\n</code></pre>"},{"location":"usage/tools/","title":"Function Calling with Tools","text":"<p>Chimeric provides a powerful function calling system that allows AI models to execute Python functions during conversations. This enables models to perform actions, fetch data, or interact with external systems.</p> <p>Provider Reliability</p> <p>Function calling reliability varies significantly by provider. OpenAI, Anthropic, and Google are among the most reliable for tool use. Other providers may have inconsistent function calling behavior, parameter parsing issues, or limited tool support. For production applications requiring reliable function calling, we recommend using these proven providers.</p>"},{"location":"usage/tools/#overview","title":"Overview","text":"<p>Function calling (also known as \"tool use\") allows LLM models to call predefined Python functions based on the conversation context. Chimeric automatically:</p> <ul> <li>Registers Python functions as tools</li> <li>Generates parameter schemas from type hints</li> <li>Parses multiple docstring formats for descriptions</li> <li>Handles tool execution and response formatting</li> </ul>"},{"location":"usage/tools/#tool-registration","title":"Tool Registration","text":""},{"location":"usage/tools/#basic-registration","title":"Basic Registration","text":"<p>Use the <code>@tool()</code> decorator to register functions:</p> <pre><code>from chimeric import Chimeric\n\nclient = Chimeric()\n\n@client.tool()\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\n\n    Args:\n        city: Name of the city to get weather for\n\n    Returns:\n        Weather description string\n    \"\"\"\n    # Your weather API logic here\n    return f\"Sunny, 75\u00b0F in {city}\"\n\n# Function is now available to all models\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What's the weather in San Francisco?\"\n)\n</code></pre>"},{"location":"usage/tools/#custom-names-and-descriptions","title":"Custom Names and Descriptions","text":"<p>Override default names and descriptions:</p> <pre><code>@client.tool(name=\"fetch_stock_data\", description=\"Retrieve real-time stock information\")\ndef get_stock_price(symbol: str, include_history: bool = False) -&gt; dict:\n    \"\"\"Get stock price and optional historical data.\"\"\"\n    return {\n        \"symbol\": symbol,\n        \"price\": 150.25,\n        \"history\": [] if not include_history else [140, 145, 150]\n    }\n</code></pre>"},{"location":"usage/tools/#supported-docstring-formats","title":"Supported Docstring Formats","text":"<p>Chimeric supports three popular docstring formats for automatic parameter documentation:</p>"},{"location":"usage/tools/#google-style-recommended","title":"Google Style (Recommended)","text":"<pre><code>@client.tool()\ndef analyze_data(data: list[float], method: str = \"mean\") -&gt; dict:\n    \"\"\"Analyze numerical data using statistical methods.\n\n    Args:\n        data: List of numerical values to analyze\n        method: Statistical method to use ('mean', 'median', 'mode')\n\n    Returns:\n        Dictionary containing analysis results\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"usage/tools/#numpy-style","title":"NumPy Style","text":"<pre><code>@client.tool()\ndef process_image(image_path: str, resize: bool = True) -&gt; str:\n    \"\"\"Process an image file with optional resizing.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to the image file to process\n    resize : bool, optional\n        Whether to resize the image, default True\n\n    Returns\n    -------\n    str\n        Path to the processed image file\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"usage/tools/#sphinx-style","title":"Sphinx Style","text":"<pre><code>@client.tool()\ndef send_email(recipient: str, subject: str, body: str = \"\") -&gt; bool:\n    \"\"\"Send an email message.\n\n    :param recipient: Email address of the recipient\n    :param subject: Subject line of the email\n    :param body: Email body content, optional\n    :returns: True if email was sent successfully\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"usage/tools/#type-system-support","title":"Type System Support","text":"<p>Chimeric automatically converts Python type hints to JSON schemas for models:</p>"},{"location":"usage/tools/#basic-types","title":"Basic Types","text":"<pre><code>@client.tool()\ndef basic_types_example(\n    text: str,           # \u2192 \"string\" \n    number: int,         # \u2192 \"integer\"\n    decimal: float,      # \u2192 \"number\"\n    flag: bool          # \u2192 \"boolean\"\n) -&gt; str:\n    \"\"\"Example of basic type support.\"\"\"\n    return f\"{text}: {number}, {decimal}, {flag}\"\n</code></pre>"},{"location":"usage/tools/#complex-types","title":"Complex Types","text":"<pre><code>@client.tool()\ndef complex_types_example(\n    items: list[str],              # \u2192 array of strings\n    metadata: dict[str, int],      # \u2192 object with integer values\n    optional_param: str | None = None,  # \u2192 optional string\n    tags: list[str] = None        # \u2192 optional array\n) -&gt; dict:\n    \"\"\"Example of complex type support.\"\"\"\n    return {\n        \"items\": items or [],\n        \"metadata\": metadata or {},\n        \"tags\": tags or []\n    }\n</code></pre>"},{"location":"usage/tools/#union-and-optional-types","title":"Union and Optional Types","text":"<pre><code>from typing import Union\n\n@client.tool()\ndef flexible_input(\n    value: Union[str, int],        # \u2192 accepts string or integer\n    optional_flag: bool | None = None,  # \u2192 optional boolean\n    default_list: list[str] = None     # \u2192 optional string array\n) -&gt; str:\n    \"\"\"Handle flexible input types.\"\"\"\n    return f\"Processed: {value}\"\n</code></pre>"},{"location":"usage/tools/#tool-management","title":"Tool Management","text":""},{"location":"usage/tools/#accessing-registered-tools","title":"Accessing Registered Tools","text":"<pre><code># Get all registered tools\nall_tools = client.tools\nprint(f\"Registered {len(all_tools)} tools:\")\nfor tool in all_tools:\n    print(f\"- {tool.name}: {tool.description}\")\n</code></pre>"},{"location":"usage/tools/#manual-tool-control","title":"Manual Tool Control","text":"<p>Control which tools are available for specific requests:</p> <pre><code># Register multiple tools\n@client.tool()\ndef get_weather(city: str) -&gt; str:\n    return f\"Weather in {city}\"\n\n@client.tool()\ndef get_stock_price(symbol: str) -&gt; float:\n    return 150.25\n\n# Use specific tools only\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What's the weather in NYC? What is the stock price of AAPL?\",\n    tools=[client.tools[0]],  # Only weather tool\n    auto_tool=False  # Don't auto-include other tools\n)\n\nprint(response)\n\n# Use all tools (default behavior)\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What's the weather in NYC? What is the stock price of AAPL?\",\n    auto_tool=True  # Automatically includes all registered tools\n)\n\nprint(response)\n</code></pre>"},{"location":"usage/tools/#the-auto_tool-parameter","title":"The auto_tool Parameter","text":"<p>The <code>auto_tool</code> parameter controls automatic tool inclusion:</p> <pre><code># auto_tool=True (default): Use all registered tools if none specified\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What can you help me with?\",\n    auto_tool=True  # All registered tools available\n)\n\n# auto_tool=False: Only use explicitly provided tools\nresponse = client.generate(\n    model=\"gpt-4o\", \n    messages=\"Get weather for Boston\",\n    tools=[weather_tool],  # Only this tool\n    auto_tool=False       # Don't add other registered tools\n)\n\n# Explicit tools override auto_tool behavior\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"Send an email\",\n    tools=[email_tool],   # Only email tool, regardless of auto_tool\n    auto_tool=True       # Ignored when tools are explicitly provided\n)\n</code></pre>"},{"location":"usage/tools/#best-practices","title":"Best Practices","text":""},{"location":"usage/tools/#function-design","title":"Function Design","text":"<ol> <li>Clear Descriptions: Write descriptive docstrings explaining what the function does</li> <li>Type Hints: Always use type hints for automatic schema generation</li> <li>Parameter Documentation: Document each parameter's purpose and format</li> <li>Error Handling: Handle errors gracefully and return meaningful messages</li> </ol> <pre><code>@client.tool()\ndef fetch_user_profile(user_id: str, include_preferences: bool = False) -&gt; dict:\n    \"\"\"Fetch user profile information from the database.\n\n    Args:\n        user_id: Unique identifier for the user (UUID format)\n        include_preferences: Whether to include user preference settings\n\n    Returns:\n        Dictionary containing user profile data, or error message if user not found\n    \"\"\"\n    try:\n        # Validate user_id format\n        if not user_id or len(user_id) &lt; 8:\n            return {\"error\": \"Invalid user_id format\"}\n\n        # Fetch user data\n        profile = get_user_from_db(user_id)\n        if not profile:\n            return {\"error\": f\"User {user_id} not found\"}\n\n        # Add preferences if requested\n        if include_preferences:\n            profile[\"preferences\"] = get_user_preferences(user_id)\n\n        return profile\n\n    except Exception as e:\n        return {\"error\": f\"Failed to fetch user profile: {str(e)}\"}\n</code></pre>"},{"location":"usage/tools/#provider-compatibility","title":"Provider Compatibility","text":"<p>Not all providers support function calling. Check provider capabilities:</p> <pre><code># Check if provider supports tools\nif \"openai\" in client.available_providers:\n    caps = client.get_capabilities(\"openai\") \n    if caps.tools:\n        print(\"OpenAI supports function calling\")\n\n# Use tools only with compatible providers\nresponse = client.generate(\n    model=\"gpt-4o\",  # OpenAI supports tools\n    messages=\"Process this data for me\",\n    # tools will be used automatically\n)\n</code></pre>"},{"location":"usage/tools/#async-function-support","title":"Async Function Support","text":"<p>Function calling works with both sync and async generation:</p> <pre><code>import asyncio\n\nasync def main():\n    # Async generation with tools\n    response = await client.agenerate(\n        model=\"gpt-4o\",\n        messages=\"What's the current time?\",\n        auto_tool=True\n    )\n    print(response.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"usage/tools/#streaming-with-tools","title":"Streaming with Tools","text":"<p>Function calling works seamlessly with streaming responses, allowing you to see both AI-generated text and tool execution results in real-time:</p> <pre><code>from chimeric import Chimeric\nfrom datetime import datetime\n\nclient = Chimeric()\n\n@client.tool()\ndef get_current_time() -&gt; str:\n    \"\"\"Get the current time.\"\"\"\n    now = datetime.now()\n    return f\"Current time: {now.strftime('%Y-%m-%d %H:%M:%S UTC')}\"\n\n@client.tool()\ndef tell_joke() -&gt; str:\n    \"\"\"Tell a programming joke.\"\"\"\n    return \"Why do programmers prefer dark mode? Because light attracts bugs! \ud83d\udc1b\"\n\n# Stream with tools enabled\nstream = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What time is it and then tell me a programming joke?\",\n    stream=True\n)\n\nprint(\"Streaming response with tools:\\n\")\n\nfor chunk in stream:\n    if chunk.delta:\n        print(chunk.delta, end=\"\", flush=True)\n\n    if chunk.finish_reason:\n        print(f\"\\n\\nStream finished: {chunk.finish_reason}\")\n</code></pre> <p>This enables real-time interaction where users can see the AI's response being generated while tools are being called and executed in the background.</p>"},{"location":"usage/tools/#complete-example","title":"Complete Example","text":"<pre><code>from chimeric import Chimeric\nfrom datetime import datetime\n\nclient = Chimeric()\n\n@client.tool()\ndef get_current_time(timezone: str = \"UTC\") -&gt; str:\n    \"\"\"Get the current time in specified timezone.\n\n    Args:\n        timezone: Timezone name (e.g., 'UTC', 'EST', 'PST')\n\n    Returns:\n        Current time as formatted string\n    \"\"\"\n    # Simple implementation for demo\n    now = datetime.now()\n    return f\"Current time ({timezone}): {now.strftime('%Y-%m-%d %H:%M:%S')}\"\n\n@client.tool()\ndef calculate_tip(bill_amount: float, tip_percentage: float = 18.0) -&gt; dict:\n    \"\"\"Calculate tip amount and total bill.\n\n    Args:\n        bill_amount: Total bill amount before tip\n        tip_percentage: Tip percentage (default 18%)\n\n    Returns:\n        Dictionary with tip amount and total\n    \"\"\"\n    tip_amount = bill_amount * (tip_percentage / 100)\n    total = bill_amount + tip_amount\n\n    return {\n        \"bill_amount\": bill_amount,\n        \"tip_percentage\": tip_percentage,\n        \"tip_amount\": round(tip_amount, 2),\n        \"total\": round(total, 2)\n    }\n\n# Use the tools\nresponse = client.generate(\n    model=\"gpt-4o\",\n    messages=\"What time is it and help me calculate a 20% tip on a $85 bill\"\n)\n\nprint(response.content)\n# Model will call both functions and provide a complete response\n</code></pre> <p>This comprehensive function calling system makes it easy to extend AI models with custom capabilities while maintaining type safety and clear documentation.</p>"}]}